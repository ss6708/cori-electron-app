# Database Schema Design for Cori RAG++ System

This document outlines the database schema design for Cori's RAG++ memory retrieval system, focusing on the vector database collections, metadata structure, and relational database tables for user preferences and feedback.

## 1. Vector Database Schema (Chroma DB)

Chroma DB will be used as the primary vector database for storing embeddings of financial knowledge. The database will be organized into multiple collections based on financial domains.

### 1.1 Collection Structure

```
cori_knowledge/
├── lbo_knowledge/         # Leveraged Buyout expertise
├── ma_knowledge/          # Mergers & Acquisitions expertise
├── debt_modeling/         # Debt modeling expertise
└── private_lending/       # Private lending expertise
```

### 1.2 Document Schema

Each document in Chroma DB will have the following structure:

```python
{
    "id": "doc_12345",                  # Unique document ID
    "text": "EBITDA is calculated...",  # Document content
    "embedding": [...],                 # Vector embedding (generated by Chroma)
    "metadata": {
        # Core metadata fields
        "domain": "financial_metrics",  # Financial domain
        "type": "concept",              # Content type
        "subtype": "financial_ratio",   # Content subtype
        "importance": "high",           # Importance level
        
        # Type-specific metadata
        "concept_name": "EBITDA",       # For concepts
        "formula_name": null,           # For formulas
        "example_type": null,           # For examples
        "practice_area": null,          # For best practices
        
        # Contextual metadata
        "related_concepts": ["EBIT", "Net Income", "Cash Flow"],
        "applicable_industries": ["all"],
        "complexity_level": "intermediate",
        
        # Source metadata
        "source": "manual_entry",
        "source_details": {
            "author": "financial_expert",
            "creation_date": "2025-03-13T12:00:00Z"
        },
        
        # System metadata
        "last_updated": "2025-03-13T12:00:00Z",
        "version": 1,
        "confidence": 0.95,
        "usage_count": 0,
        "feedback_score": null
    }
}
```

### 1.3 Type-Specific Metadata

Different types of financial knowledge will have specialized metadata:

#### 1.3.1 Concept Metadata

```python
{
    "type": "concept",
    "concept_name": "EBITDA",
    "definition_summary": "Earnings Before Interest, Taxes, Depreciation, and Amortization",
    "related_concepts": ["EBIT", "Net Income", "Cash Flow"],
    "importance": "high"
}
```

#### 1.3.2 Formula Metadata

```python
{
    "type": "formula",
    "formula_name": "EBITDA Calculation",
    "formula_notation": "EBITDA = Net Income + Interest + Taxes + Depreciation + Amortization",
    "variables": ["Net Income", "Interest", "Taxes", "Depreciation", "Amortization"],
    "usage_context": "Financial statement analysis",
    "excel_implementation": "=NetIncome+Interest+Taxes+Depreciation+Amortization"
}
```

#### 1.3.3 Example Metadata

```python
{
    "type": "example",
    "example_type": "LBO Model",
    "industry": "Software",
    "transaction_size": "mid_market",
    "complexity_level": "intermediate",
    "key_assumptions": ["5.5x Entry Multiple", "6.0x Exit Multiple", "5-year holding period"]
}
```

#### 1.3.4 Best Practice Metadata

```python
{
    "type": "best_practice",
    "practice_area": "LBO Debt Sizing",
    "applicability": "Software companies",
    "importance": "high",
    "rationale": "Software companies with recurring revenue can support higher leverage"
}
```

## 2. Relational Database Schema (SQLite)

A relational database will be used to store user preferences, feedback, and system configuration. SQLite is chosen for simplicity and ease of integration.

### 2.1 User Preferences Table

```sql
CREATE TABLE user_preferences (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    preference_type TEXT NOT NULL,  -- 'modeling', 'analysis', 'visualization', 'workflow'
    category TEXT NOT NULL,         -- 'lbo', 'ma', 'debt', 'private_lending'
    key TEXT NOT NULL,              -- Preference identifier
    value TEXT NOT NULL,            -- Preference value (JSON string)
    metadata TEXT,                  -- Additional metadata (JSON string)
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(preference_type, category, key)
);

CREATE INDEX idx_user_preferences_type_category ON user_preferences(preference_type, category);
```

### 2.2 Feedback Table

```sql
CREATE TABLE feedback (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    feedback_type TEXT NOT NULL,    -- 'response', 'correction', 'implicit'
    message_id TEXT,                -- Associated message ID
    session_id TEXT,                -- Session ID
    rating_overall INTEGER,         -- Overall rating (1-5)
    rating_accuracy INTEGER,        -- Accuracy rating (1-5)
    rating_relevance INTEGER,       -- Relevance rating (1-5)
    rating_completeness INTEGER,    -- Completeness rating (1-5)
    rating_clarity INTEGER,         -- Clarity rating (1-5)
    feedback_text TEXT,             -- Textual feedback
    context TEXT,                   -- Context information (JSON string)
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_feedback_message_id ON feedback(message_id);
CREATE INDEX idx_feedback_session_id ON feedback(session_id);
```

### 2.3 Corrections Table

```sql
CREATE TABLE corrections (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    feedback_id INTEGER,            -- Associated feedback ID
    correction_type TEXT NOT NULL,  -- 'formula', 'concept', 'example', 'best_practice'
    original_content TEXT NOT NULL, -- Original content
    corrected_content TEXT NOT NULL, -- Corrected content
    explanation TEXT,               -- Explanation for correction
    status TEXT DEFAULT 'pending',  -- 'pending', 'approved', 'rejected'
    knowledge_entry_id TEXT,        -- Associated knowledge entry ID
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (feedback_id) REFERENCES feedback(id)
);

CREATE INDEX idx_corrections_feedback_id ON corrections(feedback_id);
CREATE INDEX idx_corrections_knowledge_entry_id ON corrections(knowledge_entry_id);
```

### 2.4 Knowledge Gaps Table

```sql
CREATE TABLE knowledge_gaps (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    topic TEXT NOT NULL,            -- Gap topic
    domain TEXT NOT NULL,           -- Financial domain
    confidence REAL NOT NULL,       -- Confidence in gap identification (0-1)
    evidence_count INTEGER DEFAULT 0, -- Number of evidence points
    impact TEXT,                    -- 'low', 'medium', 'high'
    suggested_action TEXT,          -- 'knowledge_expansion', 'correction', 'refinement'
    status TEXT DEFAULT 'identified', -- 'identified', 'in_progress', 'resolved'
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_knowledge_gaps_domain ON knowledge_gaps(domain);
CREATE INDEX idx_knowledge_gaps_status ON knowledge_gaps(status);
```

### 2.5 Tool Registry Table

```sql
CREATE TABLE tools (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    tool_id TEXT UNIQUE NOT NULL,   -- Unique tool identifier
    name TEXT NOT NULL,             -- Human-readable tool name
    category TEXT NOT NULL,         -- 'excel', 'financial', 'data', 'visualization'
    subcategory TEXT NOT NULL,      -- Specific subcategory
    description TEXT NOT NULL,      -- Tool description
    input_schema TEXT NOT NULL,     -- JSON schema for inputs
    output_schema TEXT NOT NULL,    -- JSON schema for outputs
    constraints TEXT,               -- Tool constraints (JSON string)
    examples TEXT,                  -- Usage examples (JSON string)
    related_tools TEXT,             -- Related tool IDs (JSON array)
    fallback_tools TEXT,            -- Fallback tool IDs (JSON array)
    version TEXT NOT NULL,          -- Tool version
    author TEXT,                    -- Tool author
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_tools_category ON tools(category);
CREATE INDEX idx_tools_subcategory ON tools(subcategory);
```

### 2.6 Tool Executions Table

```sql
CREATE TABLE tool_executions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    execution_id TEXT UNIQUE NOT NULL, -- Unique execution identifier
    tool_id TEXT NOT NULL,           -- Tool ID
    inputs TEXT NOT NULL,            -- Execution inputs (JSON string)
    outputs TEXT,                    -- Execution outputs (JSON string)
    status TEXT DEFAULT 'pending',   -- 'pending', 'running', 'success', 'error'
    error_message TEXT,              -- Error message if status is 'error'
    context TEXT,                    -- Execution context (JSON string)
    execution_time REAL,             -- Execution time in seconds
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (tool_id) REFERENCES tools(tool_id)
);

CREATE INDEX idx_tool_executions_tool_id ON tool_executions(tool_id);
CREATE INDEX idx_tool_executions_status ON tool_executions(status);
```

## 3. Embedding Strategy

### 3.1 Embedding Model

OpenAI's `text-embedding-3-large` model will be used for generating embeddings with the following configuration:

```python
EMBEDDING_CONFIG = {
    "model": "text-embedding-3-large",
    "dimensions": 3072,              # Full dimensions for maximum quality
    "encoding_format": "float"       # Use float encoding for precision
}
```

### 3.2 Chunking Strategy

Documents will be chunked using the following strategy:

```python
CHUNKING_CONFIG = {
    "chunk_size": 1200,              # Target chunk size in tokens
    "chunk_overlap": 200,            # Overlap between chunks
    "length_function": len_in_tokens, # Function to measure token length
    "separators": ["\n\n", "\n", ". ", ", ", " "] # Chunk break points in order
}
```

### 3.3 Domain-Specific Chunking

Different financial domains will have specialized chunking strategies:

#### 3.3.1 LBO Knowledge Chunking

```python
LBO_CHUNKING_CONFIG = {
    "chunk_size": 1500,              # Larger chunks for complex LBO concepts
    "chunk_overlap": 250,            # More overlap for context preservation
    "special_separators": [
        "# Returns Analysis",
        "# Debt Sizing",
        "# Exit Valuation",
        "# Operational Projections"
    ]
}
```

#### 3.3.2 M&A Knowledge Chunking

```python
MA_CHUNKING_CONFIG = {
    "chunk_size": 1200,
    "chunk_overlap": 200,
    "special_separators": [
        "# Synergy Analysis",
        "# Accretion/Dilution",
        "# Transaction Structure",
        "# Pro Forma Analysis"
    ]
}
```

## 4. Database Integration

### 4.1 Chroma DB Integration

```python
from chromadb import Client, Settings
from chromadb.config import Settings

def initialize_chroma_db():
    """Initialize Chroma DB client and collections."""
    client = Client(Settings(
        chroma_db_impl="duckdb+parquet",
        persist_directory="./data/chroma_db"
    ))
    
    # Create collections for each domain
    collections = {
        "lbo_knowledge": client.get_or_create_collection(
            name="lbo_knowledge",
            metadata={"domain": "lbo", "description": "LBO modeling knowledge"}
        ),
        "ma_knowledge": client.get_or_create_collection(
            name="ma_knowledge",
            metadata={"domain": "ma", "description": "M&A modeling knowledge"}
        ),
        "debt_modeling": client.get_or_create_collection(
            name="debt_modeling",
            metadata={"domain": "debt", "description": "Debt modeling knowledge"}
        ),
        "private_lending": client.get_or_create_collection(
            name="private_lending",
            metadata={"domain": "private_lending", "description": "Private lending knowledge"}
        )
    }
    
    return client, collections
```

### 4.2 SQLite Integration

```python
import sqlite3
import json
from pathlib import Path

def initialize_sqlite_db():
    """Initialize SQLite database with schema."""
    db_path = Path("./data/cori_rag.db")
    db_path.parent.mkdir(parents=True, exist_ok=True)
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # Create tables (using schema definitions from above)
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS user_preferences (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            preference_type TEXT NOT NULL,
            category TEXT NOT NULL,
            key TEXT NOT NULL,
            value TEXT NOT NULL,
            metadata TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(preference_type, category, key)
        )
    """)
    
    # Create other tables...
    
    conn.commit()
    return conn
```

## 5. Data Migration and Seeding

### 5.1 Knowledge Base Seeding

Initial knowledge base seeding will be performed using:

```python
def seed_knowledge_base(collections):
    """Seed the knowledge base with initial financial knowledge."""
    # LBO knowledge
    lbo_knowledge = [
        {
            "id": "lbo_concept_001",
            "text": "Leveraged Buyout (LBO) is an acquisition strategy...",
            "metadata": {
                "domain": "lbo",
                "type": "concept",
                "concept_name": "Leveraged Buyout",
                "importance": "high",
                "complexity_level": "intermediate"
            }
        },
        # Additional LBO knowledge entries...
    ]
    
    collections["lbo_knowledge"].add(
        ids=[entry["id"] for entry in lbo_knowledge],
        documents=[entry["text"] for entry in lbo_knowledge],
        metadatas=[entry["metadata"] for entry in lbo_knowledge]
    )
    
    # Seed other collections...
```

### 5.2 Tool Registry Seeding

Initial tool registry seeding:

```python
def seed_tool_registry(conn):
    """Seed the tool registry with initial tools."""
    cursor = conn.cursor()
    
    tools = [
        {
            "tool_id": "excel_set_formula",
            "name": "Set Excel Cell Formula",
            "category": "excel",
            "subcategory": "cell_operations",
            "description": "Sets a formula in a specified Excel cell or range",
            "input_schema": json.dumps({
                "type": "object",
                "required": ["cell_reference", "formula"],
                "properties": {
                    "cell_reference": {
                        "type": "string",
                        "description": "Cell reference (e.g., 'A1') or range (e.g., 'A1:B5')"
                    },
                    "formula": {
                        "type": "string",
                        "description": "Excel formula to set (without leading =)"
                    },
                    "sheet_name": {
                        "type": "string",
                        "description": "Name of the worksheet (defaults to active sheet)"
                    }
                }
            }),
            "output_schema": json.dumps({
                "type": "object",
                "properties": {
                    "status": {
                        "type": "string",
                        "enum": ["success", "error"]
                    },
                    "message": {
                        "type": "string"
                    }
                }
            }),
            "constraints": json.dumps({
                "requires_excel": True,
                "execution_time": "fast"
            }),
            "version": "1.0.0",
            "author": "Cori Development Team"
        },
        # Additional tools...
    ]
    
    for tool in tools:
        cursor.execute("""
            INSERT OR REPLACE INTO tools
            (tool_id, name, category, subcategory, description, input_schema, 
             output_schema, constraints, version, author)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            tool["tool_id"], tool["name"], tool["category"], tool["subcategory"],
            tool["description"], tool["input_schema"], tool["output_schema"],
            tool["constraints"], tool["version"], tool["author"]
        ))
    
    conn.commit()
```

## 6. Performance Considerations

### 6.1 Indexing Strategy

Chroma DB will be configured with the following indexing strategy:

```python
INDEXING_CONFIG = {
    "hnsw:M": 16,                   # Number of bi-directional links
    "hnsw:efConstruction": 200,     # Size of the dynamic candidate list
    "hnsw:efSearch": 100            # Size of the dynamic candidate list at query time
}
```

### 6.2 Caching Strategy

```python
CACHE_CONFIG = {
    "embedding_cache": {
        "max_size": 1000,           # Maximum number of cached embeddings
        "ttl": 3600                 # Time-to-live in seconds (1 hour)
    },
    "query_cache": {
        "max_size": 100,            # Maximum number of cached query results
        "ttl": 300                  # Time-to-live in seconds (5 minutes)
    }
}
```

### 6.3 Sharding Strategy

For future scalability, the following sharding strategy will be implemented:

```python
SHARDING_CONFIG = {
    "shard_by": "domain",           # Shard by financial domain
    "max_documents_per_shard": 10000, # Maximum documents per shard
    "replication_factor": 1         # Number of replicas (increase for production)
}
```

## 7. Backup and Recovery

### 7.1 Backup Strategy

```python
BACKUP_CONFIG = {
    "schedule": "daily",            # Backup frequency
    "retention_period": 30,         # Number of days to retain backups
    "backup_path": "./data/backups", # Backup storage location
    "compression": True             # Whether to compress backups
}
```

### 7.2 Recovery Procedures

```python
def restore_from_backup(backup_date):
    """Restore databases from backup."""
    backup_path = f"./data/backups/{backup_date}"
    
    # Restore Chroma DB
    chroma_backup = f"{backup_path}/chroma_db"
    chroma_target = "./data/chroma_db"
    shutil.rmtree(chroma_target, ignore_errors=True)
    shutil.copytree(chroma_backup, chroma_target)
    
    # Restore SQLite DB
    sqlite_backup = f"{backup_path}/cori_rag.db"
    sqlite_target = "./data/cori_rag.db"
    shutil.copy2(sqlite_backup, sqlite_target)
    
    print(f"Restored databases from backup dated {backup_date}")
```

## 8. Conclusion

This database schema design provides a comprehensive framework for storing and retrieving financial knowledge, user preferences, and feedback in Cori's RAG++ system. By combining the vector capabilities of Chroma DB with the relational structure of SQLite, the system can efficiently manage both semantic similarity searches and structured data operations.
